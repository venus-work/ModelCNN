{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the data\n"
      ],
      "metadata": {
        "id": "2G70DoLYFZr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS331/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoKB6_PsVM29",
        "outputId": "aa59fb4a-95e5-41f7-fcb5-ab8efabbbd4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf datalan1/\n",
        "!unzip datalan1.zip"
      ],
      "metadata": {
        "id": "GXgaeQrkZflu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import json\n",
        "\n",
        "# json_folder = '/content/drive/MyDrive/CS331/datalan1/train/labels'\n",
        "\n",
        "# for jsonfile in os.listdir(json_folder):\n",
        "#     data = None\n",
        "#     json_path = os.path.join(json_folder, jsonfile)\n",
        "#     with open(json_path, 'r') as f:\n",
        "#         data = json.load(f)\n",
        "\n",
        "#     if 'drone' in data['imagePath']:\n",
        "#         data['imagePath'] = data['imagePath'][10:]\n",
        "\n",
        "#     # Ghi lại dữ liệu JSON đã chỉnh sửa vào tệp\n",
        "#     with open(json_path, 'w') as f:\n",
        "#         json.dump(data, f, indent=4)"
      ],
      "metadata": {
        "id": "RubH1fEpoyId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prelims"
      ],
      "metadata": {
        "id": "EAlXgAzmUhbn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fD3fY-XxRyAl"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "import cv2\n",
        "\n",
        "# data denpendencies\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# define model dependencies\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 cycle of machine learning mode\n",
        "1. Prepare the data\n",
        "2. Define the model\n",
        "3. Train model\n",
        "4. Evaluate the model"
      ],
      "metadata": {
        "id": "Mv08zG29SIAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## hyperparameters"
      ],
      "metadata": {
        "id": "JoEQGMNYFyIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "kDHWYi003mEE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "MQs6EcIfUgda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## # crawl data"
      ],
      "metadata": {
        "id": "3RKG7xT5F5Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS331/datalan1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCDKfcN8V_P_",
        "outputId": "1c5871e7-a35b-4ac5-9d25-4db0c654eecb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS331/datalan1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get label & data from jsonfile"
      ],
      "metadata": {
        "id": "IYxJk__RF7m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(label):\n",
        "    return 0 if label == 'bird' else 1"
      ],
      "metadata": {
        "id": "wP4qAs0rcT0q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(data):\n",
        "    y = [\n",
        "        float(get_label(data['shapes'][0]['label'])),\n",
        "        data['shapes'][0]['points'][0][0],\n",
        "        data['shapes'][0]['points'][0][1],\n",
        "        data['shapes'][0]['points'][1][0],\n",
        "        data['shapes'][0]['points'][1][1],\n",
        "        ]\n",
        "    y1 = torch.tensor(y)\n",
        "    return  y1"
      ],
      "metadata": {
        "id": "0nE1Jtvsdvt5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BirdDroneDataset(Dataset):\n",
        "    def __init__(self, img_path, label_path):\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_to_tensor = transforms.ToTensor()\n",
        "        self.samples = []\n",
        "\n",
        "        for jsonfile in os.listdir(label_path):\n",
        "            data = None\n",
        "\n",
        "            with open(os.path.join(label_path, jsonfile), 'r') as f:\n",
        "                data = json.load(f)\n",
        "                x = Image.open(os.path.join(img_path, data['imagePath']))\n",
        "\n",
        "\n",
        "\n",
        "            x =  self.img_to_tensor(x)\n",
        "\n",
        "            y = preprocess(data)\n",
        "\n",
        "\n",
        "            self.samples.append([x, y])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]"
      ],
      "metadata": {
        "id": "Is9xIogTSHbl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get dataset"
      ],
      "metadata": {
        "id": "QfpPv3GwGDyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = BirdDroneDataset('/content/drive/MyDrive/CS331/datalan1/train/images', '/content/drive/MyDrive/CS331/datalan1/train/labels')"
      ],
      "metadata": {
        "id": "axAzfdg8iN0k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = BirdDroneDataset('/content/drive/MyDrive/CS331/datalan1/val/images', '/content/drive/MyDrive/CS331/datalan1/val/labels')"
      ],
      "metadata": {
        "id": "eCDX2KMOmCXL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get dataloader"
      ],
      "metadata": {
        "id": "wkmmBP3MGHjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "CnCTT9EvmVuq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check dataload\n"
      ],
      "metadata": {
        "id": "wwBm6mPBQ8X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/drive/MyDrive/CS331/datalan1/val/images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9uIQkI7v1b5",
        "outputId": "0dbdccab-9eeb-4e12-eff8-afa217c8e746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS331/datalan1/val/images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_dataset)\n",
        "print(val_dataset[1][1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QeefuVSY_dz",
        "outputId": "ee4ed56e-8583-46ce-b53f-decda363aef5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  0.,   1.,  28., 223., 204.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y = [1.266454545, 2.544557, 3.4, 4.8454545, 5.1]\n",
        "\n",
        "# # Chuyển danh sách thành tensor\n",
        "# y_tensor = torch.tensor(y)\n",
        "# print(y_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxqcnnzz2rBF",
        "outputId": "6f3b60e2-c316-4ac0-b4b0-4ff2c611a9d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2665, 2.5446, 3.4000, 4.8455, 5.1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Model"
      ],
      "metadata": {
        "id": "GislXq8Sma0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, stride=2, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, stride=2, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, stride=2, kernel_size=3),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # output_shape = 32*128*6*6\n",
        "        # => shape = 128*6*6 (num_features)\n",
        "        shape = 4608\n",
        "        # b,5 -> xuống 5\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(shape, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 2)\n",
        "\n",
        "        )\n",
        "\n",
        "        self.boundingbox_regression = nn.Sequential(\n",
        "            nn.Linear(shape, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 4),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "        c = self.classifier(x)\n",
        "        c = F.sigmoid(c)\n",
        "\n",
        "        b = self.boundingbox_regression(x)\n",
        "\n",
        "\n",
        "        return c, b\n"
      ],
      "metadata": {
        "id": "BfruVv2Cmdkx"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "zvewgUmwzDrh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class TripletMarginLoss(nn.Module):\n",
        "#     def __init__(self, margin=1.0):\n",
        "#         super(TripletMarginLoss, self).__init__()\n",
        "#         self.margin = margin\n",
        "\n",
        "#     def forward(self, anchor, positive, negative):\n",
        "#         distance_positive = torch.norm(anchor - positive, dim=1)\n",
        "#         distance_negative = torch.norm(anchor - negative, dim=1)\n",
        "#         loss = torch.relu(distance_positive - distance_negative + self.margin)\n",
        "#         return torch.mean(loss)\n",
        "\n",
        "# margin_loss = TripletMarginLoss(margin=1.0)"
      ],
      "metadata": {
        "id": "vUNh13fh6kgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion2 = nn.MSELoss()\n"
      ],
      "metadata": {
        "id": "oQKBiDQVthXr"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "DtsxFnL8yhr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, (X, y) in enumerate(train_loader):\n",
        "        X = X.to(torch.float32).to(device)\n",
        "        y_true = y.to(torch.float32).to(device)\n",
        "\n",
        "        scores = model(X)\n",
        "\n",
        "        # y_true = y_true.type(torch.LongTensor)\n",
        "        loss1 = criterion(scores[0], y_true[:,0].type(torch.LongTensor))\n",
        "\n",
        "        loss2 = criterion2(scores[1], y_true[:,1:] )\n",
        "\n",
        "        loss = (loss1 + loss2)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "          # Tính accuracy\n",
        "        predicted_classes = torch.argmax(scores[0], dim=1)\n",
        "        correct_predictions += torch.sum(predicted_classes == y_true[:,0]).item()\n",
        "        total_predictions += len(y_true)\n",
        "\n",
        "        accuracy = correct_predictions / total_predictions\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = sum(losses) / len(losses)\n",
        "\n",
        "    print(f\"Epoch {epoch}, Avg Loss: {epoch_loss:.2f}, Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "w_qKsHXpygrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90e8f028-4975-43f9-a84a-707a752b5f00"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Avg Loss: 1240.26, Accuracy: 0.58\n",
            "Epoch 1, Avg Loss: 1198.79, Accuracy: 0.57\n",
            "Epoch 2, Avg Loss: 1170.46, Accuracy: 0.59\n",
            "Epoch 3, Avg Loss: 1090.87, Accuracy: 0.60\n",
            "Epoch 4, Avg Loss: 1061.86, Accuracy: 0.59\n",
            "Epoch 5, Avg Loss: 1014.06, Accuracy: 0.59\n",
            "Epoch 6, Avg Loss: 1020.83, Accuracy: 0.59\n",
            "Epoch 7, Avg Loss: 994.09, Accuracy: 0.60\n",
            "Epoch 8, Avg Loss: 981.07, Accuracy: 0.59\n",
            "Epoch 9, Avg Loss: 968.14, Accuracy: 0.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model ()"
      ],
      "metadata": {
        "id": "aq-3lxeqGcEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS331\n",
        "torch.save(model, 'best_model_ver4.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx94O-1BU5gN",
        "outputId": "30562e9b-3494-42a4-ea6f-30ad2035a2f4"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Model"
      ],
      "metadata": {
        "id": "n63vFesizha6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "e6DtOazpGfSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('best_model_ver4.pth')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "vvF5bXm0zi-1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b68495be-688b-46b6-f50c-4a262761bd47"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyModel(\n",
              "  (cnn): Sequential(\n",
              "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2))\n",
              "    (7): ReLU()\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=4608, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=2, bias=True)\n",
              "  )\n",
              "  (boundingbox_regression): Sequential(\n",
              "    (0): Linear(in_features=4608, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## get evalution dataset\n"
      ],
      "metadata": {
        "id": "3eH0qVlCG4Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evalution_dataset = BirdDroneDataset('/content/drive/MyDrive/CS331/datalan1/test/images', '/content/drive/MyDrive/CS331/datalan1/test/labels')"
      ],
      "metadata": {
        "id": "RYW-QhrPFP-1"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataloader = DataLoader(evalution_dataset)"
      ],
      "metadata": {
        "id": "fciCGf6_OKY7"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## function calculate IoU\n"
      ],
      "metadata": {
        "id": "qkqKM-Z-RH3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(box1, box2):\n",
        "    # box1 và box2 là hai bounding box, mỗi bounding box là một tuple hoặc danh sách (x1, y1, x2, y2).\n",
        "    # (x1, y1) là tọa độ góc trái trên và (x2, y2) là tọa độ góc dưới phải.\n",
        "\n",
        "    # Tính diện tích của phần giao\n",
        "    x1_inter = max(box1[0], box2[0])\n",
        "    y1_inter = max(box1[1], box2[1])\n",
        "    x2_inter = min(box1[2], box2[2])\n",
        "    y2_inter = min(box1[3], box2[3])\n",
        "\n",
        "    width_inter = max(0, x2_inter - x1_inter)\n",
        "    height_inter = max(0, y2_inter - y1_inter)\n",
        "\n",
        "    area_inter = width_inter * height_inter\n",
        "\n",
        "    # Tính tổng diện tích của cả hai bounding box\n",
        "    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    # Tính IoU\n",
        "    iou = area_inter / (area_box1 + area_box2 - area_inter)\n",
        "\n",
        "    return iou"
      ],
      "metadata": {
        "id": "tDYQn6SMf3iy"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "pre_dict = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in evaluation_dataloader:\n",
        "        c, b = model(images)\n",
        "\n",
        "        for pred_bbox, true_bbox in zip(b, labels[:1,:]):\n",
        "            iou = calculate_iou(pred_bbox, true_bbox)\n",
        "\n",
        "        if float(c[0][1]) >= 0.5:\n",
        "            z = 1\n",
        "        else:\n",
        "            z =0\n",
        "\n",
        "        pre_dict.append([b, iou,z])\n",
        "\n",
        "print(pre_dict)"
      ],
      "metadata": {
        "id": "NarqeiFMWXKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bcaeaa4-beff-457a-e2d1-a0fa1e043e14"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[tensor([[ 25.8173,  46.4946, 215.7916, 183.8678]]), tensor(0.0852), 1], [tensor([[ 22.2117,  39.7100, 184.8625, 156.7229]]), tensor(0.0833), 1], [tensor([[ 24.6486,  44.5700, 205.9707, 174.2914]]), tensor(0.0391), 1], [tensor([[ 25.1147,  45.0502, 208.8032, 177.3105]]), tensor(0.), 1], [tensor([[ 24.9485,  44.9349, 208.5853, 177.0536]]), tensor(0.0651), 1], [tensor([[ 21.9805,  39.7021, 183.6854, 156.0166]]), tensor(0.2049), 1], [tensor([[ 26.2586,  47.2228, 218.9911, 186.4964]]), tensor(0.1882), 1], [tensor([[ 23.4915,  42.2928, 194.3531, 164.6998]]), tensor(0.2842), 1], [tensor([[ 25.8750,  46.5816, 215.4660, 183.1142]]), tensor(0.), 1], [tensor([[ 24.8291,  44.4918, 206.7155, 174.8765]]), tensor(0.), 1], [tensor([[ 24.7185,  44.0646, 204.9480, 175.9872]]), tensor(0.1903), 1], [tensor([[ 24.8077,  44.6272, 207.5300, 177.1365]]), tensor(0.1036), 1], [tensor([[ 25.6390,  46.2548, 213.9659, 181.9021]]), tensor(0.0261), 1], [tensor([[ 28.6856,  51.7050, 238.7184, 203.7378]]), tensor(0.), 1], [tensor([[ 25.5129,  45.9100, 212.9172, 181.0240]]), tensor(0.), 1], [tensor([[ 25.8738,  46.8694, 215.5035, 182.8583]]), tensor(0.0842), 1], [tensor([[ 24.5289,  43.9163, 203.5351, 174.0655]]), tensor(0.1389), 1], [tensor([[ 20.1636,  36.0288, 167.7560, 142.6558]]), tensor(0.1794), 1], [tensor([[ 25.3975,  45.4985, 211.8617, 180.0655]]), tensor(0.0235), 1], [tensor([[ 23.3792,  41.7861, 193.6953, 165.6951]]), tensor(0.0418), 1], [tensor([[ 19.5687,  35.0884, 162.6952, 138.5184]]), tensor(0.2314), 1], [tensor([[ 26.9849,  48.4783, 225.8788, 192.4213]]), tensor(0.0723), 1], [tensor([[ 26.1574,  46.7433, 217.4445, 185.0323]]), tensor(0.), 1], [tensor([[ 24.8734,  43.8260, 204.6789, 174.3598]]), tensor(0.0485), 1], [tensor([[ 22.9125,  41.5018, 191.1690, 163.3635]]), tensor(0.2944), 1], [tensor([[ 24.5619,  44.0329, 203.3092, 172.7463]]), tensor(0.1598), 1], [tensor([[ 24.5075,  44.0628, 203.5471, 173.1229]]), tensor(0.1918), 1], [tensor([[ 24.7773,  45.1179, 207.7683, 176.3756]]), tensor(0.1943), 1], [tensor([[ 23.3534,  41.7283, 194.0616, 165.7270]]), tensor(0.1103), 1], [tensor([[ 24.0711,  43.0437, 200.3508, 170.8221]]), tensor(0.1650), 1], [tensor([[ 25.1662,  45.1966, 209.4622, 178.4097]]), tensor(0.1427), 1], [tensor([[ 21.2070,  38.0208, 175.5385, 149.6269]]), tensor(0.1593), 1], [tensor([[ 24.1112,  42.5421, 198.7557, 168.5449]]), tensor(0.1597), 1], [tensor([[ 24.5438,  44.1784, 203.7366, 173.4057]]), tensor(0.1521), 1], [tensor([[ 26.0616,  46.9903, 216.2764, 183.6126]]), tensor(0.1901), 1], [tensor([[ 23.6029,  42.6727, 197.2505, 167.3324]]), tensor(0.1465), 1], [tensor([[ 28.0648,  50.7129, 235.1949, 200.4810]]), tensor(0.), 1], [tensor([[ 22.4352,  40.2925, 186.9865, 159.5367]]), tensor(0.2141), 1], [tensor([[ 25.6869,  45.6079, 211.9785, 179.9357]]), tensor(0.), 1], [tensor([[ 23.9613,  43.1921, 198.6637, 167.8665]]), tensor(0.1710), 1], [tensor([[ 25.7302,  45.7589, 212.6774, 180.3670]]), tensor(0.0177), 1], [tensor([[ 23.2915,  41.5142, 192.7088, 164.3099]]), tensor(0.2596), 1], [tensor([[ 22.8458,  40.9895, 188.6043, 159.8780]]), tensor(0.2791), 1], [tensor([[ 23.9302,  42.8498, 199.1015, 169.0182]]), tensor(0.0136), 1], [tensor([[ 21.1018,  38.0033, 175.8755, 149.1128]]), tensor(0.1713), 1], [tensor([[ 25.0797,  45.4976, 209.6814, 178.3774]]), tensor(0.0622), 1], [tensor([[ 26.7659,  47.7371, 222.8888, 189.4574]]), tensor(0.), 1], [tensor([[ 24.2974,  43.1395, 201.7136, 171.9071]]), tensor(0.0851), 1], [tensor([[ 25.9836,  46.6194, 216.3180, 184.2281]]), tensor(0.1822), 1], [tensor([[ 26.5035,  47.8076, 220.4823, 187.4930]]), tensor(0.1194), 1], [tensor([[ 24.4311,  43.5169, 203.0457, 172.5724]]), tensor(0.), 1], [tensor([[ 24.6922,  44.0027, 205.2522, 175.0882]]), tensor(0.), 1], [tensor([[ 24.3112,  43.2602, 201.5344, 172.3717]]), tensor(0.), 1], [tensor([[ 22.6741,  40.4322, 187.8482, 159.9316]]), tensor(0.), 1], [tensor([[ 26.3210,  47.2408, 219.5192, 187.3921]]), tensor(0.), 1], [tensor([[ 23.2872,  42.0408, 195.2530, 166.1675]]), tensor(0.1667), 1], [tensor([[ 23.8824,  42.8154, 198.5557, 169.0093]]), tensor(0.0620), 1], [tensor([[ 23.2782,  41.0038, 192.3301, 164.8363]]), tensor(0.0274), 1], [tensor([[ 22.1456,  39.2610, 182.7128, 155.6793]]), tensor(0.0873), 1], [tensor([[ 24.9569,  45.0024, 208.3925, 178.5002]]), tensor(0.1070), 1], [tensor([[ 28.8340,  51.3955, 240.1426, 204.7520]]), tensor(0.), 1], [tensor([[ 24.1736,  42.9746, 199.5352, 170.3305]]), tensor(0.0482), 1], [tensor([[ 23.4474,  41.5577, 194.2744, 166.2141]]), tensor(0.0019), 1], [tensor([[ 25.8348,  46.0523, 214.5256, 182.5649]]), tensor(0.), 1], [tensor([[ 27.2383,  48.3997, 226.7560, 193.6114]]), tensor(0.0712), 1], [tensor([[ 25.6063,  45.6435, 213.0197, 182.0120]]), tensor(0.0862), 1], [tensor([[ 26.9130,  48.1757, 223.8286, 191.2048]]), tensor(0.0281), 1], [tensor([[ 24.4353,  43.4643, 202.4935, 172.4641]]), tensor(0.0776), 1], [tensor([[ 23.3214,  41.4115, 193.7302, 165.2935]]), tensor(0.), 1], [tensor([[ 26.5741,  47.3429, 219.8749, 187.5478]]), tensor(0.), 1], [tensor([[ 26.0035,  46.5113, 216.5431, 184.9702]]), tensor(0.), 1], [tensor([[ 28.0967,  50.3890, 235.9860, 201.6268]]), tensor(0.), 1], [tensor([[ 26.2783,  46.8450, 218.4378, 185.9030]]), tensor(0.1189), 1], [tensor([[ 25.7894,  45.7152, 214.1293, 182.9669]]), tensor(0.), 1], [tensor([[ 22.7916,  40.3426, 189.6600, 162.7106]]), tensor(0.0191), 1], [tensor([[ 26.4100,  47.1835, 220.5322, 188.5667]]), tensor(0.0276), 1], [tensor([[ 24.3210,  43.1419, 202.3221, 171.9353]]), tensor(0.), 1], [tensor([[ 25.1188,  45.0047, 208.5516, 178.1375]]), tensor(0.1465), 1], [tensor([[ 21.9451,  39.0791, 182.3682, 156.4930]]), tensor(0.0844), 1], [tensor([[ 24.3437,  43.8181, 202.7293, 173.0104]]), tensor(0.2627), 1], [tensor([[ 23.1116,  41.4453, 192.8604, 163.5020]]), tensor(0.), 1], [tensor([[ 26.7685,  47.9641, 222.9503, 190.4803]]), tensor(0.0537), 1], [tensor([[ 25.9481,  46.3157, 215.7225, 184.0281]]), tensor(0.), 1], [tensor([[ 26.4769,  47.4277, 220.4265, 187.6104]]), tensor(0.), 1], [tensor([[ 25.7157,  46.0589, 213.0661, 182.2171]]), tensor(0.0276), 1], [tensor([[ 22.1337,  39.5285, 183.3251, 155.4831]]), tensor(0.), 1], [tensor([[ 22.8675,  40.8617, 189.7361, 161.2217]]), tensor(0.0483), 1], [tensor([[ 21.4722,  38.6179, 179.4761, 152.4713]]), tensor(0.), 1], [tensor([[ 23.6050,  42.4681, 196.3333, 167.1407]]), tensor(0.1667), 1], [tensor([[ 23.4829,  42.0182, 195.1076, 166.2040]]), tensor(0.0199), 1], [tensor([[ 24.5196,  43.5603, 201.6955, 172.1609]]), tensor(0.), 1], [tensor([[ 21.9391,  39.3191, 181.8334, 154.6291]]), tensor(0.1793), 1], [tensor([[ 21.0857,  37.8695, 176.3754, 150.6111]]), tensor(0.4082), 1], [tensor([[ 23.7202,  42.6090, 198.7259, 168.6573]]), tensor(0.0654), 1], [tensor([[ 20.5546,  37.0197, 172.5022, 147.3350]]), tensor(0.0075), 1], [tensor([[ 22.5176,  40.2111, 187.8186, 160.1280]]), tensor(0.2209), 1], [tensor([[ 21.5675,  38.8840, 181.2737, 154.6473]]), tensor(0.1433), 1], [tensor([[ 23.6135,  42.3727, 195.5336, 166.0008]]), tensor(0.0616), 1], [tensor([[ 24.0984,  43.3923, 201.0099, 170.7665]]), tensor(0.), 1], [tensor([[ 23.8797,  43.1173, 200.5479, 170.5799]]), tensor(0.1136), 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test ảnh\n"
      ],
      "metadata": {
        "id": "XnM5nC4YRU15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def plot_image_with_bboxes(image,predicted_bbox, predicted_class):\n",
        "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
        "    ax.imshow(image, cmap='gray')\n",
        "\n",
        "    # Vẽ bounding box dự đoán (màu đỏ) và gắn tên object\n",
        "    if predicted_bbox is not None:\n",
        "        x1, x2, y1, y2  = predicted_bbox\n",
        "\n",
        "        predicted_rect = patches.Rectangle(\n",
        "            (x1, x2), y1, y2,\n",
        "            linewidth=3, edgecolor='black', facecolor='none', label='Predicted'\n",
        "        )\n",
        "        ax.add_patch(predicted_rect)\n",
        "        ax.text(x1, x2+y2, predicted_class, color='black', backgroundcolor='none', fontsize=16)\n",
        "    print(\"Predicted_bbox:\", predicted_bbox)\n",
        "\n",
        "    # Hiển thị\n",
        "    plt.legend(handles=[predicted_rect])\n",
        "    plt.axis('on')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "1eFGtu8D4Eat"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "threshold = 0.5\n",
        "total_im = []\n",
        "\n",
        "\n",
        "# Danh sách đường dẫn tệp tin ảnh\n",
        "image_paths = [\"/content/drive/MyDrive/CS331/datalan1/test/images\"]\n",
        "\n",
        "i = 0\n",
        "predicted_class = None\n",
        "for path in image_paths:\n",
        "    x1 = pre_dict[i][0][0][0]\n",
        "    x2 = pre_dict[i][0][0][1]\n",
        "    y1 = pre_dict[i][0][0][2]\n",
        "    y2 = pre_dict[i][0][0][3]\n",
        "\n",
        "    if pre_dict[i][2] == 1:\n",
        "        predicted_class = 'Drone'\n",
        "    else:\n",
        "        predicted_class = 'Bird'\n",
        "\n",
        "    plot_image_with_bboxes(path, pre_dict[i][0][0], predicted_class)\n",
        "\n",
        "    i += 1\n",
        ""
      ],
      "metadata": {
        "id": "f2-fc4OxgIVu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}